<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  code {
  	padding: 2px;
  	color: #c7254e;
  	background-color: #f9f2f4;
  	border-radius: 4px;
  }

  .tableClass { 
  	border: 1px solid black;
  	border-collapse: collapse;
  	padding: 10px 8px;
  	text-align: left;
  }

</style>
<title>CS284 Final Project Proposal</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


	<h1 align="middle">CS284 Final Project Proposal</h1>
	<br>

	<h1 align="middle">Neural Inverse Rendering of Animatable Actors from Videos</h1>
	<br>

	<h1 align="middle">Summary</h1>

	<p>
		In this project, we aim to recover 3D animatable human avatars from multi-view video inputs. These human avatars are composed of a canonical 3D shape, skeleton skinning weights, pose-independent albedo, and pose-dependent shading/deformation effects. Once these attributes are recovered, we can then animate the avatar and render it in novel poses and novel viewing angles.
	</p>
	<br>

	<h1 align="middle">Team Members</h1>

	<p align="middle">
		Erich Liang, 
		Ethan Weber, 
		Nianxu Wang,
		Ruilong Li
	</p>
	<br>


	<h1 align="middle">Problem Description</h1>

	<p>
		This project will be built on top of Ruilong’s recent submission “TAVA: Template-free Animatable Volumetric Actors,” which extends MipNeRF [1] to animatable objects using Neural Radiance Fields (NeRF) [2]. The pipeline of TAVA is shown in Figure 1 (below). TAVA already succeeds at extracting a 3D shape and skeleton skinning weights which extrapolate well to poses not seen during training (out-of-distribution). However, the input TAVA needs to extract this info includes multi-view video as well as per-frame ground truth skeleton pose information, the latter of which might not be readily available to the common person. To make rendering 3D animatable actors more accessible, we plan to decrease TAVA’s dependence on skeleton pose information. Specifically, we aim to show that 1) approximate skeleton pose data can be inferred directly from multi-view video; and 2) inferred skeleton pose data is sufficient to produce high quality extrapolations of 3D shape and skeleton skinning weights. These contributions will more firmly place TAVA into the realm of inverse graphics for computer vision. 
	</p>

	<div align="middle">
		<img src="images/tava.png" align="middle" width="500px">
		<figcaption><i>Figure 1 TAVA</i></figcaption>
	</div>

	<p>
		The inputs to TAVA are a set of multi-view videos of an actor with per-frame 3D poses. Through neural inverse rendering, we are able to disentangle the geometry, appearance, skinning weights, and shading effects of this actor and create a volumetric digital avatar of this actor in the canonical space. It can then be used for novel-pose rendering and editing.
	</p>
	<br>

	
	<h1 align="middle">Goals and Deliverables</h1>

	<p>
		The overall plan for this project is to create a formulation of the physically-based neural rendering that is differentiable to many attributes of the actor, including geometry, appearance (texture), skinning weights, and the skeleton pose. With that, we would be able to perform inverse rendering from multi-view rgb images, thus should be able to disentangle and optimize for all those aforementioned attributes through gradient descent. The plan is to build on top of our recent project TAVA, which already establishes a neural rendering process that can be used to reconstruct the geometry, appearance and skinning weights of an actor assuming known skeleton pose. This project would focus more on the differentiable formulation of the skeleton pose in the rendering pipeline, making it optimizable through inverse rendering. Yet this is a challenging problem as the skinning weights and skeleton pose are usually tightly coupled and hard to disentangle. Our baseline plan is to assume that we have an inaccurate initialization of the skeleton pose of the actor (which we plan to perturb the groundtruth skeleton pose), and jointly optimize this skeleton pose together with other attributes to make it accurate. After accomplishing this, our advanced plan is to increase the magnitude of the perturbation to the input pose and in the end we would like to be able to optimize the input skeleton pose from scratch without any reliance on a good initialization. The deliverables from this project are expected to cover the following:
		
		<ul>
			<li>We plan to show that given an inaccurate initial skeleton pose estimation of the actor, we would be able to correct the skeleton pose, recover the 3D attributes of the actor, and render realistic images of the actor in multiple views. The preliminary work TAVA is expected to fail without accurate skeleton pose.</li>
			<li>We further plan to animate the reconstructed actor using novel poses and render it into images to show our formulation is able to disentangle the skinning weights with the skeleton pose from rgb images.</li>
		</ul>

		Furthermore, we also plan to speedup the TAVA rendering pipeline by introducing the most recent voxel-based rendering techniques, such as instant-ngp [3], PlenOctree [4], Plenoxels [5]. This will be evaluated by comparing the number of rays that can be rendered per second.

	</p>
	<br>

    
	<h1 align="middle">Schedule</h1>

	<p>
		Week 1: <br>
		<ul>
			<li>Begin reducing TAVA model training time using approaches such as instant-ngp [3], PlenOctree [4], Plenoxels [5]</li>
			<li>Brainstorm potential deep learning approaches for inferring skeleton pose from either inaccurate or missing skeleton data</li>
			<li>Work on TAVA codebase to enable quick experimentation with different pose inference models and approaches</li>
		</ul>
	</p>

	<p>
		Week 2: <br>
		<ul>
			<li>Wrap up TAVA model training speed up efforts</li>
			<li>Run initial pose optimization experiments, with the goal of reconstructing 3D shape and skeleton skinning weights given input of multi-view video and slightly noisy / perturbed per-frame skeleton data</li>
		</ul>
	</p>

	<p>
		Week 3: <br>
		<ul>
			<li>Increase experiment difficulty by increasing the magnitude of noise / perturbations of per-frame skeleton data</li>
			<li>If previous experiments with perturbed per-frame skeleton data go well, begin experimenting with reconstructing 3D shape and skeleton skinning weights given input of multi-view video and only canonical skeleton data (frame independent)</li>
		</ul>
	</p>

	<p>
		Week 4: <br>
		<ul>
			<li>Wrap up pose-estimation experiments</li>
			<li>Finish writing final paper</li>
		</ul>
	</p>

	<br>


	<h1 align="middle">Resources</h1>

	<p>
		Training NeRF-related models can be very compute intensive, and we plan on using GPU-enabled machines provided by Prof. Angjoo Kanazawa and Prof. Ren Ng. Simultaneously, we will also investigate integrating recent advances in NeRF that can help cut down on training time, such as instant-ngp, PlenOctree, and Plenoxels. The software platforms we intend on using include python, pytorch, and Blender. The main dataset we will be using is ZJU-mocap, and the main codebase we will be working off of is TAVA (currently not released to the public).
	</p>
	<br>

	<h3 align="middle">Bibliography</h3>
	<p>
		[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In International Conference on Computer Vision, 2021.
		<br><br>

		[2] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405–421. Springer, 2020.
		<br><br>

		[3] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. CoRR, abs/2201.05989, 2022.
		<br><br>

		[4] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. CoRR, abs/2103.14024, 2021.
		<br><br>

		[5] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks, 2021.
	</p>
	<br>


</body>

</html>