<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  code {
  	padding: 2px;
  	color: #c7254e;
  	background-color: #f9f2f4;
  	border-radius: 4px;
  }

  .tableClass { 
  	border: 1px solid black;
  	border-collapse: collapse;
  	padding: 10px 8px;
  	text-align: left;
  }

</style>
<title>CS284 Final Project Proposal</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


	<h1 align="middle">CS284 Final Project Proposal</h1>
	<br>

	<h1 align="middle">Neural Inverse Rendering of Animatable Actors from Videos</h1>
	<br>

	<h1 align="middle">Summary</h1>

	<p>
		In this project, we aim to recover 3D animatable human avatars from multi-view video inputs. These human avatars are composed of a canonical 3D shape, skeleton skinning weights, pose-independent albedo, and pose-dependent shading/deformation effects. Once these attributes are recovered, we can then animate the avatar and render it in novel poses and novel viewing angles.
	</p>
	<br>

	<h1 align="middle">Team Members</h1>

	<p align="middle">
		Erich Liang, 
		Ethan Weber, 
		Nianxu Wang,
		Ruilong Li
	</p>
	<br>


	<h1 align="middle">Problem Description</h1>

	<p>
		This project will be built on top of Ruilong’s recent submission “TAVA: Template-free Animatable Volumetric Actors,” which extends MipNeRF [1] to animatable objects using Neural Radiance Fields (NeRF) [2]. The pipeline of TAVA is shown in Figure 1 (below). TAVA already succeeds at extracting a 3D shape and skeleton skinning weights which extrapolate well to poses not seen during training (out-of-distribution). However, the input TAVA needs to extract this info includes multi-view video as well as per-frame ground truth skeleton pose information, the latter of which might not be readily available to the common person. To make rendering 3D animatable actors more accessible, we plan to decrease TAVA’s dependence on skeleton pose information. Specifically, we aim to show that 1) approximate skeleton pose data can be inferred directly from multi-view video; and 2) inferred skeleton pose data is sufficient to produce high quality extrapolations of 3D shape and skeleton skinning weights. These contributions will more firmly place TAVA into the realm of inverse graphics for computer vision. 
	</p>

	<div align="middle">
		<img src="images/tava.png" align="middle" width="500px">
		<figcaption><i>Figure 1 TAVA</i></figcaption>
	</div>

	<p>
		The inputs to TAVA are a set of multi-view videos of an actor with per-frame 3D poses. Through neural inverse rendering, we are able to disentangle the geometry, appearance, skinning weights, and shading effects of this actor and create a volumetric digital avatar of this actor in the canonical space. It can then be used for novel-pose rendering and editing.
	</p>
	<br>

	
	<h1 align="middle">Goals and Deliverables</h1>

	<p>
		This is the most important part of your proposal. You should carefully think through what you are trying to accomplish, what results you are going for, and why you think you can accomplish those goals. For example:
    
    Since this is a graphics class you will likely define the kind of images you will create (e.g. including a photo of a new lighting effect you will simulate).
    
    If you are working on an interactive system, describe what demo you will create.
   
   	Define how you will measure the quality / performance of your system (e.g. graphs showing speedup, or quantifying accuracy). It may not be possible to define precise target metrics at this time, but we encourage you to try.
    
    What questions do you plan to answer with your analysis?
    
    You should break this section into two parts: (1) what you plan to deliver, and (2) what you hope to deliver. In (1), describe what you believe you must accomplish to have a successful project and achieve the grade you expect (i.e. your baseline plan -- planning for some unexpected problems would make sense). In (2), describe what you hope to achieve if things go well and you get ahead of schedule (your aspirational plan).
	</p>
	<br>

    
	<h1 align="middle">Schedule</h1>

	<p>
		In this section you should organize and plan the tasks and subtasks that your team will execute. Since presentations are ~4 weeks from the due-date of the proposal, you should include a set of tasks for every week.
	</p>
	<br>


	<h1 align="middle">Resources</h1>

	<p>
		List what resources, e.g. books, papers and/or online resources that are references for your project. List the computing platform, hardware and software resources that you will use for your project. You have a wide latitude here to use what you have access to, but be aware that you will have to support and trouble-shoot on your platform yourselves. If you are starting from an existing piece of code or system, describe and provide a pointer to it here.
	</p>
	<br>


</body>

</html>